{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7dae93-d124-4b77-a241-47e1d318d98f",
   "metadata": {},
   "source": [
    "# KI Assistiertes Lehren\n",
    "\n",
    "# Tutorial: Inhalte erstellen per KI\n",
    "\n",
    "In diesem Tutorial werden folgende Inhalte vorgestellt:\n",
    "\n",
    "- Erstellung von Inhalten per einfachem Prompt\n",
    "- Erstellung von Inhalten per One-Shot Prompting\n",
    "- Erstellung von Inhalten aus anderen Quellen per RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Notwendige Kenntnisse:\n",
    "\n",
    "- Python\n",
    "\n",
    "Mein Kontakt:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138aa598-1b86-45fe-a1d1-8651aa53d643",
   "metadata": {},
   "source": [
    "Als erstes installieren wir benötigte Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8cc7b-6518-4a0b-b81a-61f47111abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-core langchain-huggingface huggingface-hub text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccce48a-87d2-4044-ab98-376e233cbda5",
   "metadata": {},
   "source": [
    "Nun stellen wir eine Verbindung zu einem LLM her. Hier kann jedes gehostete LLM verwendet werden, z.B. auch Chat-GPT. Wir verwenden Modelle von unserem TH Lübeck Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e73d8a-e722-4b9c-8dff-e2e2dbc0fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten\n",
    "HOST = \"https://chat-{model}.llm.mylab.th-luebeck.dev\"\n",
    "MODELLE = [\"slim\", \"default\", \"large\"]\n",
    "modell_name = MODELLE[2]\n",
    "\n",
    "# Globale Variable\n",
    "inference_api_url = HOST.format(model=modell_name)\n",
    "print(inference_api_url)\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795f828-b87e-46dc-89ac-4006d52e0a99",
   "metadata": {},
   "source": [
    "Langchain ist eine sehr nützliche Bibliothek zum Aufsetzen von KI-Pipelines. Mehr dazu findet Ihr auch hier: \n",
    "\n",
    "* [Tutorial zu Langchain von THL Kollegen Nane Kratzke, Keno Teppris](https://git.mylab.th-luebeck.de/gpu/tutorials/-/blob/c0cb6721bbd7eb4e092e1f30c6d7af8ea6413891/32-langchain.ipynb)\n",
    "* [Tutorial zu Langchain+RAG von THL Kollegen Nane Kratzke](https://git.mylab.th-luebeck.de/gpu/tutorials/-/blob/c0cb6721bbd7eb4e092e1f30c6d7af8ea6413891/33-rag.ipynb)\n",
    "* [Langchain](https://python.langchain.com/v0.1/docs/get_started/introduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5cb59-9823-429f-bc4d-ed19391c8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Erstellen Sie das LLM-Objekt\n",
    "llm = HuggingFaceEndpoint(\n",
    "    # cache=None,  # Optional: Cache verwenden oder nicht\n",
    "    verbose=True,  # Ob ausführliche Ausgaben angezeigt werden sollen\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # Callbacks, wir verwenden den fürs Streaming: Raus nehmen wenn nicht streaming genutzt werden soll\n",
    "    max_new_tokens=1024,  # Die maximale Anzahl an Tokens, die generiert werden sollen\n",
    "    # top_k=2,  # Die Anzahl der Top-K Tokens, die beim Generieren berücksichtigt werden sollen\n",
    "    top_p=0.95,  # Die kumulative Wahrscheinlichkeitsschwelle beim Generieren\n",
    "    typical_p=0.95,  # Die typische Wahrscheinlichkeitsschwelle beim Generieren\n",
    "    temperature=0.1,  # Die \"Temperatur\" beim Generieren, gibt an wie\n",
    "    # repetition_penalty=None,  # Wiederholungsstrafe beim Generieren\n",
    "    # truncate=None,  # Schneidet die Eingabe-Tokens auf die gegebene Größe\n",
    "    # stop_sequences=None,  # Eine Liste von Stop-Sequenzen beim Generieren\n",
    "    #inference_server_url=inference_api_url,  # URL des Inferenzservers\n",
    "    endpoint_url=inference_api_url,\n",
    "    timeout=10,  # Timeout in Sekunden für die Verbindung zum Inferenzserver\n",
    "    streaming=True,  # Ob die Antwort gestreamt werden soll,\n",
    "    huggingfacehub_api_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee9083-fe55-4820-94b0-af92b29a53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "SYSTEM_PROMPT = \"Du bist ein hilfreicher Assistent und erstellst Materialien für Deep Learning.\" # ändere hier den System Prompt für deinen Bot.\n",
    "\n",
    "PROMPT_TEMPLATE = PromptTemplate.from_template(\n",
    "    SYSTEM_PROMPT + \"\\n\\n{history}\\nUSER:{input}\\nASSISTANT:\"\n",
    ")\n",
    "\n",
    "print(PROMPT_TEMPLATE.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb7513-ff5f-4de4-945b-49718932bfbe",
   "metadata": {},
   "source": [
    "Wir erstellen uns zusätzlich einen Speicher für unsere Konversation, um auch auf vorherige Punkte der Konversation eingehen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de20070-bedc-423b-b692-419d1ddc242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# folgende Funktionen nur zu besseren Darstellung\n",
    "#def make_bold(string):\n",
    "#    \"\"\"Makes a String bold.\"\"\"\n",
    "#    return \"\\033[1m\" +  string + \"\\033[0m\"\n",
    "\n",
    "#def print_section(title, content):\n",
    "#    \"\"\"Prints a section with a bold title and the content.\"\"\"\n",
    "#    print(f\"{make_bold(title)}\\n{content}\\n\")\n",
    "\n",
    "# Erstellen eines Memory Buffers\n",
    "memory = ConversationBufferMemory(\n",
    "    human_prefix=\"USER\",\n",
    "    ai_prefix=\"ASSISTANT\"\n",
    ")\n",
    "\n",
    "memory.clear()  # Bisherigen Chatverlauf löschen\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    prompt=PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a72c8-e276-4752-a3d7-29f1f1f4ba9d",
   "metadata": {},
   "source": [
    "## Erstellung von Folien über Latex\n",
    "\n",
    "Über einfaches Prompting lässt sich auch Latex-Code ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f7f27-042a-4eaf-acbc-183640cd4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex slide example\n",
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.invoke(\"Erstelle mir Folien zu den verschiedenen Bereichen des Deep Learnings. \\\n",
    "    Die Ausgabe soll in Latex Beamer Slides als Code erfolgen. Keine Überschriften oder Latex-Code Blöcke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0919b-5ef1-4f5b-b87a-0949b2dfc663",
   "metadata": {},
   "source": [
    "# Erstellung von Skripten aus Vorlesungsfolien\n",
    "\n",
    "Um aus bestehenden Materialien neue Materialien zu erstellen, muss die KI auf die Inhalte zugreifen können. Hierfür müssen weitere Python-Pakete installiert werden, welche diese Dokumente importierbar machen. \n",
    "\n",
    "* PDF Dokumente lassen sich über viele PDF Pakete lesen- Hier verwenden wir `PyMuPDF`\n",
    "* Word Dateien lassen sich über `docx` einlesen\n",
    "* Powerpoint lassen sich über `pptx` einlesen\n",
    "\n",
    "Nachdem Import der Dokumente können diese entweder direkt in den Prompt mit aufgenommen werden oder falls eine schnelle Suche auf den Daten ermöglicht werden soll, indiziert werden. \n",
    "\n",
    "Man kann auch direkt Powerpoint Folien erstellen. Ein Beispiel dazu findet man hier: https://github.com/leonid20000/odin-slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fa518-77fe-45a5-8ef7-617d897632a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install PyMuPDF chromadb langchain huggingface-hub --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf2fca-5694-4c44-b441-21c0a87c9dd5",
   "metadata": {},
   "source": [
    "Wer mehr Details über den Import von PDF Dateien haben möchte, kann hier nachlesen: [Langchain PDF Loader](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e376a4-032d-4676-9e16-46a26514b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# der splitter zerteilt den Text in Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800, # größe eines Chunks \n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "loader = PyMuPDFLoader(\"02_dl_supervisedlearning.pdf\", extract_images=False)\n",
    "\n",
    "docs = loader.load_and_split(text_splitter)\n",
    "# Ausgabe der Seite 2 des PDF's\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d0f2a-455e-45ec-ba5b-3809f9dc04a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## generate skript by adding text per page\n",
    "#result_text = []\n",
    "memory.clear()\n",
    "for counter, i in enumerate(docs):\n",
    "    query = \"Schreibe ein Skript zu dem folgenden Inhalt. \\\n",
    "    Erstelle einen ausführlichen Text basierend auf den Informationen der folgenden Folie. \\\n",
    "    Ergänze Inhalte, wenn die Information zu wenig ist. \\\n",
    "    Wenn die Information auf der Folie zu dünn ist, dann lasse die Folie aus. \\\n",
    "    Ergänze den Text zu deinem bisherigen Text. \\.:\\n\" + i.page_content\n",
    "    last_answer = conversation.invoke(query)\n",
    "    #result_text.append(last_answer)\n",
    "    if counter >= 7:\n",
    "        print(last_answer)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362c48a-dcd3-43d8-98a4-119a1c4d0e52",
   "metadata": {},
   "source": [
    "## Erstellung von Prüfungsfragen\n",
    "\n",
    "Sehr hilfreich ist die KI bei der Erstellung von Prüfungsfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec322f-e694-4c4f-9a61-653a5c5777ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.invoke(\"Erstelle drei Multiple-Choice Fragen zur Lernrate. Keine Überschriften. \\\n",
    "Die Ausgabe als Code und auf Deutsch. \\\n",
    "Halte dich an folgende Struktur: Frage A) Antwort B) Antwort C) Antwort D) Antwort ANSWER: A B C or D EXPLANATION: \")\n",
    "\n",
    "#last_answer = conversation.invoke(\"Erstelle drei weitere Fragen im gleichen Ausgabeformat zu Regularisierung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d54942-ba93-4a90-a7c7-169d1b9c7802",
   "metadata": {},
   "source": [
    "Es gibt verschiedene Möglichkeiten, die Prompts zu schreiben, um bestimmte Ausgabeformate zu erzwingen. Einige Beispiele werden z.B. [hier für H5P](https://h5p.org/ai) erklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acaf157-ebe2-480a-86f9-16446e152968",
   "metadata": {},
   "source": [
    "## Erstellung von XML basierten Moodle Imports\n",
    "\n",
    "Noch besser funktioniert die Ausgabe, wenn man direkt ein Template (One-Shot Prompting) verwendet. Dies wird für alle Fragen für Moodle empfohlen, da der Import als XML die reichhaltigste Variante des Inputs darstellt und viele Details mit aufgenommen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c7615-4543-4965-a8ef-665cfec6a17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.invoke(\"Erstelle Aussagen zu Regularisierungstechniken, die entweder wahr oder falsch sind. \\\n",
    "Schreibe jede Aussage an die entsprechende Stelle <shorttext>Aussage 1,2,3 oder 4</shorttext> und \\\n",
    "ersetze das Wort Aussage 1,2,3 oder 4. Adaptiere die auf jede Aussage folgende Zeilen <weight-of-col>0</weight-of-col> \\\n",
    "und schreibe dort in die erste Zeile eine 1 und in die darauffolgende Spalte eine 0, bei einer wahren Aussage. \\\n",
    "Bei einer falschen Aussage vertauscht Du die 1 und 0. Für die richtige Antwort jeweils unter Feedback ein. \\\n",
    "Nutze folgendes Template und fülle dies aus:\\\n",
    "<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> \\\n",
    "<quiz> \\\n",
    "  <question type=\\\"matrix\\\"> \\\n",
    "    <name> \\\n",
    "      <text>Batch Normalization 1</text> \\\n",
    "    </name> \\\n",
    "    <questiontext format=\\\"html\\\"> \\\n",
    "      <text><![CDATA[<p dir=\\\"ltr\\\" style=\\\"text-align: left;\\\">Taskdescription</p>]]></text> \\\n",
    "    </questiontext> \\\n",
    "    <generalfeedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "    </generalfeedback> \\\n",
    "    <defaultgrade>4</defaultgrade> \\\n",
    "    <penalty>0</penalty> \\\n",
    "    <hidden>0</hidden> \\\n",
    "    <idnumber></idnumber> \\\n",
    "    <use_dnd_ui>0</use_dnd_ui> \\\n",
    "    <row> \\\n",
    "        <shorttext>Aussage 1</shorttext> \\\n",
    "        <feedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </feedback> \\\n",
    "    </row> \\\n",
    "    <weights-of-row> \\\n",
    "    <weight-of-col>0</weight-of-col> \\\n",
    "    <weight-of-col>1</weight-of-col> \\\n",
    "    </weights-of-row> \\\n",
    "    <row> \\\n",
    "        <shorttext>Aussage 2</shorttext> \\\n",
    "        <feedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </feedback> \\\n",
    "    </row> \\\n",
    "    <weights-of-row> \\\n",
    "    <weight-of-col>1</weight-of-col> \\\n",
    "    <weight-of-col>0</weight-of-col> \\\n",
    "    </weights-of-row> \\\n",
    "    <row> \\\n",
    "        <shorttext>Aussage 3</shorttext> \\\n",
    "        <feedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </feedback> \\\n",
    "    </row> \\\n",
    "    <weights-of-row> \\\n",
    "    <weight-of-col>1</weight-of-col> \\\n",
    "    <weight-of-col>0</weight-of-col> \\\n",
    "    </weights-of-row> \\\n",
    "    <row> \\\n",
    "        <shorttext>Aussage 4</shorttext> \\\n",
    "        <feedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </feedback> \\\n",
    "    </row> \\\n",
    "    <weights-of-row> \\\n",
    "    <weight-of-col>0</weight-of-col> \\\n",
    "    <weight-of-col>1</weight-of-col> \\\n",
    "    </weights-of-row> \\\n",
    "    <col> \\\n",
    "        <shorttext>Wahr</shorttext> \\\n",
    "        <description format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </description> \\\n",
    "    </col> \\\n",
    "    <col> \\\n",
    "        <shorttext>Falsch</shorttext> \\\n",
    "        <description format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "        </description> \\\n",
    "    </col> \\\n",
    "    <grademethod>all</grademethod> \\\n",
    "    <shuffleanswers>1</shuffleanswers> \\\n",
    "    <multiple>0</multiple> \\\n",
    "    <renderer>matrix</renderer> \\\n",
    "  </question> \\\n",
    "</quiz>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0171d-d7db-4b57-a34a-90c88703aed0",
   "metadata": {},
   "source": [
    "Man kann auf diese Weise viele verschiedene Fragen für Moodle erstellen. Als nächstes ein Beispiel für eine [Drop-Down Frage](https://docs.moodle.org/404/de/Fragetyp_L%C3%BCckentextauswahl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f05914-2d7d-4ecc-9316-f73d3d8678a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.invoke(\"Erstelle eine Aussage zur Lernrate. Wir wollen eine Dropdown-Auswahlmöglichkeit erzeugen. \\\n",
    "Entferne dazu den wichtigen Teil der Aussage und schreibe den unwichtigen Teil an die Stelle STATEMENT. \\\n",
    "Ersetze in dem Template die Stellen FILL 1,2,3 mit zwei falschen vervollständigungen der Aussage und einmal mit der richigen Aussage. \\\n",
    "Vor jede falsche Vervollständigung kommt eine Tilde \\\"~\\\" und vor die richtige Vervollständigung eine Tilde und ein Gleichheitszeichen \\\"~=\\\" \\\n",
    "Die Ersetzung soll hinter dem Wort \\\"MULTICHOICE:\\\" beginnen. \\\n",
    "Schreibe jeweils direkt hinter eine Vervollständigung an die Stelle FEEDBACK 1,2,3 in dem Template eine Aussage, \\\n",
    "warum die Ergänzung richtig oder falsch ist. Mache zwischen Vervollständigung und Feedback eine Raute \\\"#\\\". \\\n",
    "<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?> \\\n",
    "<quiz> \\\n",
    "  <question type=\\\"cloze\\\"> \\\n",
    "    <name> \\\n",
    "      <text>TITLE</text> \\\n",
    "    </name> \\\n",
    "    <questiontext format=\\\"html\\\"> \\\n",
    "      <text><![CDATA[<p dir=\\\"ltr\\\" style=\\\"text-align: left;\\\">STATEMENT {1:MULTICHOICE: FILL 1#FEEDBACK 1~FILL 2#FEEDBACK 2~=FILL 3#FEEDBACK 3}<br></p><p></p>]]></text> \\\n",
    "    </questiontext> \\\n",
    "    <generalfeedback format=\\\"html\\\"> \\\n",
    "      <text></text> \\\n",
    "    </generalfeedback> \\\n",
    "    <penalty>0.3333333</penalty> \\\n",
    "    <hidden>0</hidden> \\\n",
    "    <idnumber></idnumber> \\\n",
    "  </question> \\\n",
    "</quiz>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f2beb-6ae3-4a35-a372-38e62a9061e4",
   "metadata": {},
   "source": [
    "Hier ein Beispiel für eine [Drag & Drop Frage](https://docs.moodle.org/404/de/Fragetyp_Drag-and-Drop_auf_Text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b162bd-e7ee-41d4-8a18-0faae484232e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.invoke(\"Erstelle einen Text zur Erklärung von linearer Regression. Lasse dabei Schluesselwoerter aus und ersetze diese\\\n",
    "durch aufsteigende Zahlen in doppelten eckigen Klammern \\\"[[1]]\\\". Schreibe diesen Text in folgendes Template an die Stelle TEXT. \\\n",
    "Schreibe die Schluesselwoerter in der richtigen Reihenfolge ihres Auftretens jeweils in ein dragbox xml-statement. Die Zahl der Group soll immer 1 sein. \\\n",
    "Füge am Ende noch drei weitere Schluesselworter, welche nicht exakt in den Text passen, in gleicher Weise an. \\\n",
    "Keine Zahl darf doppelt auftreten. \\\n",
    "<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\\n",
    "<quiz>\\\n",
    "  <question type=\\\"ddwtos\\\">\\\n",
    "    <name>\\\n",
    "      <text>TITLE</text>\\\n",
    "    </name>\\\n",
    "    <questiontext format=\\\"html\\\">\\\n",
    "      <text>TEXT</text>\\\n",
    "    </questiontext>\\\n",
    "    <generalfeedback format=\\\"html\\\">\\\n",
    "      <text></text> \\\n",
    "    </generalfeedback> \\\n",
    "    <defaultgrade>3</defaultgrade> \\\n",
    "    <penalty>1</penalty> \\\n",
    "    <hidden>0</hidden> \\\n",
    "    <idnumber></idnumber> \\\n",
    "    <shuffleanswers>1</shuffleanswers> \\\n",
    "    <correctfeedback format=\\\"html\\\"> \\\n",
    "      <text>Die Antwort ist richtig.</text> \\\n",
    "    </correctfeedback> \\\n",
    "    <partiallycorrectfeedback format=\\\"html\\\"> \\\n",
    "      <text>Die Antwort ist teilweise richtig.</text> \\\n",
    "    </partiallycorrectfeedback> \\\n",
    "    <incorrectfeedback format=\\\"html\\\"> \\\n",
    "      <text>Die Antwort ist falsch.</text> \\\n",
    "    </incorrectfeedback> \\\n",
    "    <shownumcorrect/> \\\n",
    "    <dragbox> \\\n",
    "      <text>FILLTEXT</text> \\\n",
    "      <group>1</group> \\\n",
    "    </dragbox> \\\n",
    "  </question> \\\n",
    "</quiz>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f723053-83c0-4512-9b10-9a9a0c23c6df",
   "metadata": {},
   "source": [
    "## Übungsaufgaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a78a0d-d91b-4c57-a3dd-a1f8bd4d3bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.run(\"Erstelle drei Übungsaufgaben im Bereich Physik zum Thema Lichtbrechung. Die Aufgaben sollen von leicht nach schwer gegliedert sein.\")\n",
    "\n",
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.run(\"Erstelle drei Übungsaufgaben für ein Jupyter Notebook. Die Beschreibung soll in Markdown und etwaiger Code in Python sein. Zeige die Ergebnisse. Gib keine extra Überschriften mit aus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7aeca-0308-4b42-bc35-a7535d4313ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "\n",
    "last_answer = conversation.run(\"Erstelle eine Python-Übungsaufgabe zum Thema Intersection over Union. \\\n",
    "    Lasse dafür entscheidenen Code aus. Die Stelle zum Bearbeiten soll klar mit \\\"Ihr Code hier\\\" gekennzeichnet sein. \\\n",
    "    Ausgabe als reiner Code ohne extra Überschriften.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0e177-7230-4295-b28c-4471081de76c",
   "metadata": {},
   "source": [
    "## AI Tutor\n",
    "\n",
    "Wir können die Dokumente aber nicht nur zur Erstellung von neuen Inhalten verwenden. Wir können diese auch für einen Chatbot verwenden. Dazu fügen wir alle Inhalte in eine leicht durchsuchbare Datenbank und stellen diese unserer KI zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d502c-c298-4c63-a1d6-96d081587e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705b58d-ceed-4b80-9359-6afee27b10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutzen der oben angegebenen Modelle durch Angabe des Endpunkts\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceHubEmbeddings(model=\"https://nomic-text-v15-embedding.llm.mylab.th-luebeck.dev\")\n",
    "\n",
    "# Wir laden ein anderes PDF\n",
    "loader = PyMuPDFLoader(\"MNG1_Blatt3_Lösungen.pdf\", extract_images=False)\n",
    "docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# Chroma Vectorstore vorbereiten (erst leeren, dann neu anlegen)\n",
    "# Falls notwendig, löschen des Vektorstores\n",
    "#    db.delete_collection()\n",
    "db.delete_collection()\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d438f-efd3-43f7-9444-ea7b9c6cd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Du bist ein Mathe-Tutor. Beantworte die Frage nur mit dem folgenden Kontext, aber gib nicht die finale Lösung der Aufgabe aus. \\\n",
    "Erkläre es so ausführlich, wie für ein kleines Kind und gib andere Beispiel dafür:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"Wie berechne ich Aufgabe 1b in dem Übungsblatt? \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
